// AWS Tutorials - Comprehensive SEO-Optimized Content
window.TUTORIALS = {};

window.TUTORIALS['lambda-api-gateway'] = {
  "title": "Build REST API with Lambda + API Gateway",
  "category": "Serverless",
  "difficulty": "Beginner",
  "duration": "10 min",
  "description": "Create a serverless REST API in 10 minutes using AWS Lambda and API Gateway.",
  "whatYouLearn": [
    "Understand build rest api with lambda + api gateway architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Lambda",
    "API Gateway",
    "Python",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Build REST API with Lambda + API Gateway</h2>\n    <p class=\"mb-4\">Serverless computing revolutionizes application development by eliminating server management, automatically scaling with demand, and charging only for actual usage. This tutorial demonstrates building production-ready serverless solutions using AWS's managed services.</p>\n    <p class=\"mb-4\"><strong>Why Serverless?</strong> Traditional server-based applications require capacity planning, OS patching, security updates, and 24/7 monitoring. Serverless eliminates these operational burdens while providing automatic scaling, built-in high availability, and pay-per-use pricing.</p>\n    <p class=\"mb-4\">In this hands-on tutorial, you'll learn industry best practices for create a serverless rest api in 10 minutes using aws lambda and api gateway.. We'll cover architecture design, implementation patterns, error handling, monitoring, and deployment automation using Infrastructure as Code.</p>\n    <p>By the end, you'll have a production-ready solution that can handle thousands of requests per second, automatically scales to zero when idle, and costs pennies to run. This pattern is used by companies processing millions of transactions daily.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing build rest api with lambda + api gateway, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir lambda_api_gateway\ncd lambda_api_gateway\n\n# Initialize SAM project\nsam init --runtime python3.11 --name lambda-api-gateway\n\n# Project structure:\n# lambda-api-gateway/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for build rest api with lambda + api gateway. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Build REST API with Lambda + API Gateway.\n    \n    This function implements create a serverless rest api in 10 minutes using aws lambda and api gateway..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for build rest api with lambda + api gateway: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Build REST API with Lambda + API Gateway - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: lambda-api-gateway\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Create a serverless REST API in 10 minutes using AWS Lambda and API Gateway.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /lambda-api-gateway\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: lambda-api-gateway\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/lambda-api-gateway'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: lambda-api-gateway-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name lambda-api-gateway-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name lambda-api-gateway-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name lambda-api-gateway-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=lambda-api-gateway-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name lambda-api-gateway-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for build rest api with lambda + api gateway.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name lambda-api-gateway-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name lambda-api-gateway-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['s3-lambda-trigger'] = {
  "title": "Process S3 Uploads with Lambda",
  "category": "Serverless",
  "difficulty": "Beginner",
  "duration": "8 min",
  "description": "Automatically process files uploaded to S3 using Lambda triggers.",
  "whatYouLearn": [
    "Understand process s3 uploads with lambda architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Lambda",
    "S3",
    "Events",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Process S3 Uploads with Lambda</h2>\n    <p class=\"mb-4\">Serverless computing revolutionizes application development by eliminating server management, automatically scaling with demand, and charging only for actual usage. This tutorial demonstrates building production-ready serverless solutions using AWS's managed services.</p>\n    <p class=\"mb-4\"><strong>Why Serverless?</strong> Traditional server-based applications require capacity planning, OS patching, security updates, and 24/7 monitoring. Serverless eliminates these operational burdens while providing automatic scaling, built-in high availability, and pay-per-use pricing.</p>\n    <p class=\"mb-4\">In this hands-on tutorial, you'll learn industry best practices for automatically process files uploaded to s3 using lambda triggers.. We'll cover architecture design, implementation patterns, error handling, monitoring, and deployment automation using Infrastructure as Code.</p>\n    <p>By the end, you'll have a production-ready solution that can handle thousands of requests per second, automatically scales to zero when idle, and costs pennies to run. This pattern is used by companies processing millions of transactions daily.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing process s3 uploads with lambda, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir s3_lambda_trigger\ncd s3_lambda_trigger\n\n# Initialize SAM project\nsam init --runtime python3.11 --name s3-lambda-trigger\n\n# Project structure:\n# s3-lambda-trigger/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for process s3 uploads with lambda. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Process S3 Uploads with Lambda.\n    \n    This function implements automatically process files uploaded to s3 using lambda triggers..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for process s3 uploads with lambda: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Process S3 Uploads with Lambda - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: s3-lambda-trigger\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Automatically process files uploaded to S3 using Lambda triggers.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /s3-lambda-trigger\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: s3-lambda-trigger\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/s3-lambda-trigger'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: s3-lambda-trigger-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name s3-lambda-trigger-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name s3-lambda-trigger-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name s3-lambda-trigger-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=s3-lambda-trigger-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name s3-lambda-trigger-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for process s3 uploads with lambda.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name s3-lambda-trigger-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name s3-lambda-trigger-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['dynamodb-crud'] = {
  "title": "DynamoDB CRUD Operations",
  "category": "Serverless",
  "difficulty": "Beginner",
  "duration": "12 min",
  "description": "Master DynamoDB operations with Python boto3 SDK.",
  "whatYouLearn": [
    "Understand dynamodb crud operations architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "DynamoDB",
    "Python",
    "NoSQL",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to DynamoDB CRUD Operations</h2>\n    <p class=\"mb-4\">Serverless computing revolutionizes application development by eliminating server management, automatically scaling with demand, and charging only for actual usage. This tutorial demonstrates building production-ready serverless solutions using AWS's managed services.</p>\n    <p class=\"mb-4\"><strong>Why Serverless?</strong> Traditional server-based applications require capacity planning, OS patching, security updates, and 24/7 monitoring. Serverless eliminates these operational burdens while providing automatic scaling, built-in high availability, and pay-per-use pricing.</p>\n    <p class=\"mb-4\">In this hands-on tutorial, you'll learn industry best practices for master dynamodb operations with python boto3 sdk.. We'll cover architecture design, implementation patterns, error handling, monitoring, and deployment automation using Infrastructure as Code.</p>\n    <p>By the end, you'll have a production-ready solution that can handle thousands of requests per second, automatically scales to zero when idle, and costs pennies to run. This pattern is used by companies processing millions of transactions daily.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing dynamodb crud operations, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir dynamodb_crud\ncd dynamodb_crud\n\n# Initialize SAM project\nsam init --runtime python3.11 --name dynamodb-crud\n\n# Project structure:\n# dynamodb-crud/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for dynamodb crud operations. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for DynamoDB CRUD Operations.\n    \n    This function implements master dynamodb operations with python boto3 sdk..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for dynamodb crud operations: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: DynamoDB CRUD Operations - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: dynamodb-crud\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Master DynamoDB operations with Python boto3 SDK.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /dynamodb-crud\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: dynamodb-crud\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/dynamodb-crud'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: dynamodb-crud-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name dynamodb-crud-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name dynamodb-crud-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name dynamodb-crud-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=dynamodb-crud-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name dynamodb-crud-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for dynamodb crud operations.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name dynamodb-crud-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name dynamodb-crud-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['eventbridge-scheduler'] = {
  "title": "Schedule Tasks with EventBridge",
  "category": "Serverless",
  "difficulty": "Beginner",
  "duration": "7 min",
  "description": "Run Lambda functions on a schedule using EventBridge rules.",
  "whatYouLearn": [
    "Understand schedule tasks with eventbridge architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "EventBridge",
    "Lambda",
    "Scheduler",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Schedule Tasks with EventBridge</h2>\n    <p class=\"mb-4\">Serverless computing revolutionizes application development by eliminating server management, automatically scaling with demand, and charging only for actual usage. This tutorial demonstrates building production-ready serverless solutions using AWS's managed services.</p>\n    <p class=\"mb-4\"><strong>Why Serverless?</strong> Traditional server-based applications require capacity planning, OS patching, security updates, and 24/7 monitoring. Serverless eliminates these operational burdens while providing automatic scaling, built-in high availability, and pay-per-use pricing.</p>\n    <p class=\"mb-4\">In this hands-on tutorial, you'll learn industry best practices for run lambda functions on a schedule using eventbridge rules.. We'll cover architecture design, implementation patterns, error handling, monitoring, and deployment automation using Infrastructure as Code.</p>\n    <p>By the end, you'll have a production-ready solution that can handle thousands of requests per second, automatically scales to zero when idle, and costs pennies to run. This pattern is used by companies processing millions of transactions daily.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing schedule tasks with eventbridge, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir eventbridge_scheduler\ncd eventbridge_scheduler\n\n# Initialize SAM project\nsam init --runtime python3.11 --name eventbridge-scheduler\n\n# Project structure:\n# eventbridge-scheduler/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for schedule tasks with eventbridge. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Schedule Tasks with EventBridge.\n    \n    This function implements run lambda functions on a schedule using eventbridge rules..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for schedule tasks with eventbridge: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Schedule Tasks with EventBridge - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: eventbridge-scheduler\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Run Lambda functions on a schedule using EventBridge rules.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /eventbridge-scheduler\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: eventbridge-scheduler\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/eventbridge-scheduler'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: eventbridge-scheduler-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name eventbridge-scheduler-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name eventbridge-scheduler-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name eventbridge-scheduler-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=eventbridge-scheduler-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name eventbridge-scheduler-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for schedule tasks with eventbridge.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name eventbridge-scheduler-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name eventbridge-scheduler-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['step-functions-workflow'] = {
  "title": "Build Workflows with Step Functions",
  "category": "Serverless",
  "difficulty": "Intermediate",
  "duration": "15 min",
  "description": "Orchestrate multiple Lambda functions into a workflow.",
  "whatYouLearn": [
    "Understand build workflows with step functions architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Step Functions",
    "Lambda",
    "Workflow",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Build Workflows with Step Functions</h2>\n    <p class=\"mb-4\">Serverless computing revolutionizes application development by eliminating server management, automatically scaling with demand, and charging only for actual usage. This tutorial demonstrates building production-ready serverless solutions using AWS's managed services.</p>\n    <p class=\"mb-4\"><strong>Why Serverless?</strong> Traditional server-based applications require capacity planning, OS patching, security updates, and 24/7 monitoring. Serverless eliminates these operational burdens while providing automatic scaling, built-in high availability, and pay-per-use pricing.</p>\n    <p class=\"mb-4\">In this hands-on tutorial, you'll learn industry best practices for orchestrate multiple lambda functions into a workflow.. We'll cover architecture design, implementation patterns, error handling, monitoring, and deployment automation using Infrastructure as Code.</p>\n    <p>By the end, you'll have a production-ready solution that can handle thousands of requests per second, automatically scales to zero when idle, and costs pennies to run. This pattern is used by companies processing millions of transactions daily.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing build workflows with step functions, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir step_functions_workflow\ncd step_functions_workflow\n\n# Initialize SAM project\nsam init --runtime python3.11 --name step-functions-workflow\n\n# Project structure:\n# step-functions-workflow/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for build workflows with step functions. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Build Workflows with Step Functions.\n    \n    This function implements orchestrate multiple lambda functions into a workflow..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for build workflows with step functions: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Build Workflows with Step Functions - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: step-functions-workflow\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Orchestrate multiple Lambda functions into a workflow.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /step-functions-workflow\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: step-functions-workflow\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/step-functions-workflow'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: step-functions-workflow-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name step-functions-workflow-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name step-functions-workflow-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name step-functions-workflow-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=step-functions-workflow-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name step-functions-workflow-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for build workflows with step functions.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name step-functions-workflow-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name step-functions-workflow-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['bedrock-text-generation'] = {
  "title": "Generate Text with Amazon Bedrock",
  "category": "AI & GenAI",
  "difficulty": "Beginner",
  "duration": "8 min",
  "description": "Use Claude or Titan models to generate AI-powered text.",
  "whatYouLearn": [
    "Understand generate text with amazon bedrock architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Bedrock",
    "GenAI",
    "Claude",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Generate Text with Amazon Bedrock</h2>\n    <p class=\"mb-4\">Generative AI is transforming how applications interact with users, process information, and create content. Amazon Bedrock provides access to state-of-the-art foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API.</p>\n    <p class=\"mb-4\"><strong>Why Amazon Bedrock?</strong> Building AI applications traditionally required managing GPU infrastructure, fine-tuning models, and handling complex ML pipelines. Bedrock eliminates this complexity by providing fully managed access to foundation models with enterprise-grade security, privacy, and compliance.</p>\n    <p class=\"mb-4\">This tutorial teaches you how to use claude or titan models to generate ai-powered text. using Bedrock's powerful APIs. You'll learn prompt engineering techniques, response streaming, error handling, cost optimization, and integration patterns for production applications.</p>\n    <p>The skills you learn here apply to building chatbots, content generation systems, document analysis tools, code assistants, and any application requiring natural language understanding or generation.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing generate text with amazon bedrock, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir bedrock_text_generation\ncd bedrock_text_generation\n\n# Initialize SAM project\nsam init --runtime python3.11 --name bedrock-text-generation\n\n# Project structure:\n# bedrock-text-generation/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for generate text with amazon bedrock. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Generate Text with Amazon Bedrock.\n    \n    This function implements use claude or titan models to generate ai-powered text..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for generate text with amazon bedrock: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Generate Text with Amazon Bedrock - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: bedrock-text-generation\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Use Claude or Titan models to generate AI-powered text.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /bedrock-text-generation\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: bedrock-text-generation\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/bedrock-text-generation'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: bedrock-text-generation-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name bedrock-text-generation-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name bedrock-text-generation-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name bedrock-text-generation-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=bedrock-text-generation-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name bedrock-text-generation-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for generate text with amazon bedrock.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name bedrock-text-generation-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name bedrock-text-generation-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['bedrock-chatbot'] = {
  "title": "Build a Chatbot with Bedrock",
  "category": "AI & GenAI",
  "difficulty": "Intermediate",
  "duration": "12 min",
  "description": "Create a conversational AI chatbot using Amazon Bedrock.",
  "whatYouLearn": [
    "Understand build a chatbot with bedrock architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Bedrock",
    "Chatbot",
    "Lambda",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Build a Chatbot with Bedrock</h2>\n    <p class=\"mb-4\">Generative AI is transforming how applications interact with users, process information, and create content. Amazon Bedrock provides access to state-of-the-art foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API.</p>\n    <p class=\"mb-4\"><strong>Why Amazon Bedrock?</strong> Building AI applications traditionally required managing GPU infrastructure, fine-tuning models, and handling complex ML pipelines. Bedrock eliminates this complexity by providing fully managed access to foundation models with enterprise-grade security, privacy, and compliance.</p>\n    <p class=\"mb-4\">This tutorial teaches you how to create a conversational ai chatbot using amazon bedrock. using Bedrock's powerful APIs. You'll learn prompt engineering techniques, response streaming, error handling, cost optimization, and integration patterns for production applications.</p>\n    <p>The skills you learn here apply to building chatbots, content generation systems, document analysis tools, code assistants, and any application requiring natural language understanding or generation.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing build a chatbot with bedrock, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir bedrock_chatbot\ncd bedrock_chatbot\n\n# Initialize SAM project\nsam init --runtime python3.11 --name bedrock-chatbot\n\n# Project structure:\n# bedrock-chatbot/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for build a chatbot with bedrock. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Build a Chatbot with Bedrock.\n    \n    This function implements create a conversational ai chatbot using amazon bedrock..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for build a chatbot with bedrock: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Build a Chatbot with Bedrock - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: bedrock-chatbot\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Create a conversational AI chatbot using Amazon Bedrock.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /bedrock-chatbot\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: bedrock-chatbot\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/bedrock-chatbot'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: bedrock-chatbot-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name bedrock-chatbot-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name bedrock-chatbot-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name bedrock-chatbot-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=bedrock-chatbot-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name bedrock-chatbot-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for build a chatbot with bedrock.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name bedrock-chatbot-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name bedrock-chatbot-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['rag-kendra-bedrock'] = {
  "title": "RAG with Kendra + Bedrock",
  "category": "AI & GenAI",
  "difficulty": "Advanced",
  "duration": "20 min",
  "description": "Build a knowledge-base chatbot using RAG architecture.",
  "whatYouLearn": [
    "Understand rag with kendra + bedrock architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "RAG",
    "Kendra",
    "Bedrock",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to RAG with Kendra + Bedrock</h2>\n    <p class=\"mb-4\">Generative AI is transforming how applications interact with users, process information, and create content. Amazon Bedrock provides access to state-of-the-art foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API.</p>\n    <p class=\"mb-4\"><strong>Why Amazon Bedrock?</strong> Building AI applications traditionally required managing GPU infrastructure, fine-tuning models, and handling complex ML pipelines. Bedrock eliminates this complexity by providing fully managed access to foundation models with enterprise-grade security, privacy, and compliance.</p>\n    <p class=\"mb-4\">This tutorial teaches you how to build a knowledge-base chatbot using rag architecture. using Bedrock's powerful APIs. You'll learn prompt engineering techniques, response streaming, error handling, cost optimization, and integration patterns for production applications.</p>\n    <p>The skills you learn here apply to building chatbots, content generation systems, document analysis tools, code assistants, and any application requiring natural language understanding or generation.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing rag with kendra + bedrock, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir rag_kendra_bedrock\ncd rag_kendra_bedrock\n\n# Initialize SAM project\nsam init --runtime python3.11 --name rag-kendra-bedrock\n\n# Project structure:\n# rag-kendra-bedrock/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for rag with kendra + bedrock. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for RAG with Kendra + Bedrock.\n    \n    This function implements build a knowledge-base chatbot using rag architecture..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for rag with kendra + bedrock: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: RAG with Kendra + Bedrock - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: rag-kendra-bedrock\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Build a knowledge-base chatbot using RAG architecture.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /rag-kendra-bedrock\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: rag-kendra-bedrock\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/rag-kendra-bedrock'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: rag-kendra-bedrock-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name rag-kendra-bedrock-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name rag-kendra-bedrock-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name rag-kendra-bedrock-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=rag-kendra-bedrock-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name rag-kendra-bedrock-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for rag with kendra + bedrock.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name rag-kendra-bedrock-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name rag-kendra-bedrock-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['prompt-engineering'] = {
  "title": "Prompt Engineering Essentials",
  "category": "AI & GenAI",
  "difficulty": "Beginner",
  "duration": "10 min",
  "description": "Master prompt design for better AI outputs.",
  "whatYouLearn": [
    "Understand prompt engineering essentials architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Prompts",
    "GenAI",
    "Best Practices",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Prompt Engineering Essentials</h2>\n    <p class=\"mb-4\">Generative AI is transforming how applications interact with users, process information, and create content. Amazon Bedrock provides access to state-of-the-art foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API.</p>\n    <p class=\"mb-4\"><strong>Why Amazon Bedrock?</strong> Building AI applications traditionally required managing GPU infrastructure, fine-tuning models, and handling complex ML pipelines. Bedrock eliminates this complexity by providing fully managed access to foundation models with enterprise-grade security, privacy, and compliance.</p>\n    <p class=\"mb-4\">This tutorial teaches you how to master prompt design for better ai outputs. using Bedrock's powerful APIs. You'll learn prompt engineering techniques, response streaming, error handling, cost optimization, and integration patterns for production applications.</p>\n    <p>The skills you learn here apply to building chatbots, content generation systems, document analysis tools, code assistants, and any application requiring natural language understanding or generation.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing prompt engineering essentials, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir prompt_engineering\ncd prompt_engineering\n\n# Initialize SAM project\nsam init --runtime python3.11 --name prompt-engineering\n\n# Project structure:\n# prompt-engineering/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for prompt engineering essentials. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Prompt Engineering Essentials.\n    \n    This function implements master prompt design for better ai outputs..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for prompt engineering essentials: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Prompt Engineering Essentials - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: prompt-engineering\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Master prompt design for better AI outputs.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /prompt-engineering\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: prompt-engineering\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/prompt-engineering'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: prompt-engineering-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name prompt-engineering-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name prompt-engineering-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name prompt-engineering-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=prompt-engineering-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name prompt-engineering-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for prompt engineering essentials.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name prompt-engineering-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name prompt-engineering-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['bedrock-agents-basics'] = {
  "title": "Build AI Agents with Bedrock Agents",
  "category": "Agentic AI",
  "difficulty": "Intermediate",
  "duration": "15 min",
  "description": "Create autonomous AI agents that can use tools and APIs.",
  "whatYouLearn": [
    "Understand build ai agents with bedrock agents architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Agents",
    "Bedrock",
    "Tools",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Build AI Agents with Bedrock Agents</h2>\n    <p class=\"mb-4\">AI Agents represent the next evolution in artificial intelligence - autonomous systems that can reason, plan, use tools, and take actions to achieve goals. Unlike simple chatbots, agents can break down complex tasks, make decisions, call APIs, and interact with external systems.</p>\n    <p class=\"mb-4\"><strong>What are AI Agents?</strong> Agents combine large language models with the ability to use tools (APIs, databases, search engines) and maintain context across multiple interactions. They can research information, analyze data, generate reports, and execute multi-step workflows autonomously.</p>\n    <p class=\"mb-4\">In this advanced tutorial, you'll build create autonomous ai agents that can use tools and apis.. We'll cover agent architecture, tool integration, memory management, error recovery, and orchestration patterns for complex workflows.</p>\n    <p>This technology powers virtual assistants, automated research systems, customer service bots, and intelligent automation platforms used by enterprises worldwide.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing build ai agents with bedrock agents, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir bedrock_agents_basics\ncd bedrock_agents_basics\n\n# Initialize SAM project\nsam init --runtime python3.11 --name bedrock-agents-basics\n\n# Project structure:\n# bedrock-agents-basics/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for build ai agents with bedrock agents. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Build AI Agents with Bedrock Agents.\n    \n    This function implements create autonomous ai agents that can use tools and apis..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for build ai agents with bedrock agents: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Build AI Agents with Bedrock Agents - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: bedrock-agents-basics\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Create autonomous AI agents that can use tools and APIs.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /bedrock-agents-basics\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: bedrock-agents-basics\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/bedrock-agents-basics'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: bedrock-agents-basics-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name bedrock-agents-basics-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name bedrock-agents-basics-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name bedrock-agents-basics-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=bedrock-agents-basics-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name bedrock-agents-basics-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for build ai agents with bedrock agents.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name bedrock-agents-basics-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name bedrock-agents-basics-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['agent-with-knowledge-base'] = {
  "title": "AI Agent with Custom Knowledge Base",
  "category": "Agentic AI",
  "difficulty": "Advanced",
  "duration": "18 min",
  "description": "Give your agent domain-specific knowledge using vector databases.",
  "whatYouLearn": [
    "Understand ai agent with custom knowledge base architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Agents",
    "Knowledge Base",
    "RAG",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to AI Agent with Custom Knowledge Base</h2>\n    <p class=\"mb-4\">AI Agents represent the next evolution in artificial intelligence - autonomous systems that can reason, plan, use tools, and take actions to achieve goals. Unlike simple chatbots, agents can break down complex tasks, make decisions, call APIs, and interact with external systems.</p>\n    <p class=\"mb-4\"><strong>What are AI Agents?</strong> Agents combine large language models with the ability to use tools (APIs, databases, search engines) and maintain context across multiple interactions. They can research information, analyze data, generate reports, and execute multi-step workflows autonomously.</p>\n    <p class=\"mb-4\">In this advanced tutorial, you'll build give your agent domain-specific knowledge using vector databases.. We'll cover agent architecture, tool integration, memory management, error recovery, and orchestration patterns for complex workflows.</p>\n    <p>This technology powers virtual assistants, automated research systems, customer service bots, and intelligent automation platforms used by enterprises worldwide.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing ai agent with custom knowledge base, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir agent_with_knowledge_base\ncd agent_with_knowledge_base\n\n# Initialize SAM project\nsam init --runtime python3.11 --name agent-with-knowledge-base\n\n# Project structure:\n# agent-with-knowledge-base/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for ai agent with custom knowledge base. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for AI Agent with Custom Knowledge Base.\n    \n    This function implements give your agent domain-specific knowledge using vector databases..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for ai agent with custom knowledge base: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: AI Agent with Custom Knowledge Base - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: agent-with-knowledge-base\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Give your agent domain-specific knowledge using vector databases.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /agent-with-knowledge-base\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: agent-with-knowledge-base\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/agent-with-knowledge-base'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: agent-with-knowledge-base-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name agent-with-knowledge-base-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name agent-with-knowledge-base-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name agent-with-knowledge-base-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=agent-with-knowledge-base-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name agent-with-knowledge-base-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for ai agent with custom knowledge base.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name agent-with-knowledge-base-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name agent-with-knowledge-base-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['multi-agent-system'] = {
  "title": "Build Multi-Agent Systems",
  "category": "Agentic AI",
  "difficulty": "Advanced",
  "duration": "22 min",
  "description": "Orchestrate multiple AI agents working together.",
  "whatYouLearn": [
    "Understand build multi-agent systems architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Agents",
    "Orchestration",
    "Bedrock",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Build Multi-Agent Systems</h2>\n    <p class=\"mb-4\">AI Agents represent the next evolution in artificial intelligence - autonomous systems that can reason, plan, use tools, and take actions to achieve goals. Unlike simple chatbots, agents can break down complex tasks, make decisions, call APIs, and interact with external systems.</p>\n    <p class=\"mb-4\"><strong>What are AI Agents?</strong> Agents combine large language models with the ability to use tools (APIs, databases, search engines) and maintain context across multiple interactions. They can research information, analyze data, generate reports, and execute multi-step workflows autonomously.</p>\n    <p class=\"mb-4\">In this advanced tutorial, you'll build orchestrate multiple ai agents working together.. We'll cover agent architecture, tool integration, memory management, error recovery, and orchestration patterns for complex workflows.</p>\n    <p>This technology powers virtual assistants, automated research systems, customer service bots, and intelligent automation platforms used by enterprises worldwide.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing build multi-agent systems, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir multi_agent_system\ncd multi_agent_system\n\n# Initialize SAM project\nsam init --runtime python3.11 --name multi-agent-system\n\n# Project structure:\n# multi-agent-system/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for build multi-agent systems. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Build Multi-Agent Systems.\n    \n    This function implements orchestrate multiple ai agents working together..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for build multi-agent systems: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Build Multi-Agent Systems - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: multi-agent-system\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Orchestrate multiple AI agents working together.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /multi-agent-system\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: multi-agent-system\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/multi-agent-system'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: multi-agent-system-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name multi-agent-system-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name multi-agent-system-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name multi-agent-system-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=multi-agent-system-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name multi-agent-system-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for build multi-agent systems.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name multi-agent-system-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name multi-agent-system-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['agent-tool-use'] = {
  "title": "AI Agents with Custom Tools",
  "category": "Agentic AI",
  "difficulty": "Intermediate",
  "duration": "14 min",
  "description": "Teach agents to use external APIs and services.",
  "whatYouLearn": [
    "Understand ai agents with custom tools architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Agents",
    "Tools",
    "API",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to AI Agents with Custom Tools</h2>\n    <p class=\"mb-4\">AI Agents represent the next evolution in artificial intelligence - autonomous systems that can reason, plan, use tools, and take actions to achieve goals. Unlike simple chatbots, agents can break down complex tasks, make decisions, call APIs, and interact with external systems.</p>\n    <p class=\"mb-4\"><strong>What are AI Agents?</strong> Agents combine large language models with the ability to use tools (APIs, databases, search engines) and maintain context across multiple interactions. They can research information, analyze data, generate reports, and execute multi-step workflows autonomously.</p>\n    <p class=\"mb-4\">In this advanced tutorial, you'll build teach agents to use external apis and services.. We'll cover agent architecture, tool integration, memory management, error recovery, and orchestration patterns for complex workflows.</p>\n    <p>This technology powers virtual assistants, automated research systems, customer service bots, and intelligent automation platforms used by enterprises worldwide.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing ai agents with custom tools, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir agent_tool_use\ncd agent_tool_use\n\n# Initialize SAM project\nsam init --runtime python3.11 --name agent-tool-use\n\n# Project structure:\n# agent-tool-use/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for ai agents with custom tools. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for AI Agents with Custom Tools.\n    \n    This function implements teach agents to use external apis and services..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for ai agents with custom tools: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: AI Agents with Custom Tools - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: agent-tool-use\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Teach agents to use external APIs and services.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /agent-tool-use\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: agent-tool-use\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/agent-tool-use'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: agent-tool-use-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name agent-tool-use-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name agent-tool-use-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name agent-tool-use-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=agent-tool-use-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name agent-tool-use-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for ai agents with custom tools.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name agent-tool-use-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name agent-tool-use-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['sam-template-basics'] = {
  "title": "AWS SAM Template Basics",
  "category": "DevOps",
  "difficulty": "Beginner",
  "duration": "10 min",
  "description": "Define serverless infrastructure as code with SAM.",
  "whatYouLearn": [
    "Understand aws sam template basics architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "SAM",
    "IaC",
    "CloudFormation",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to AWS SAM Template Basics</h2>\n    <p class=\"mb-4\">Modern DevOps practices emphasize automation, Infrastructure as Code, continuous integration/deployment, and comprehensive monitoring. AWS provides a complete suite of tools for implementing DevOps best practices at scale.</p>\n    <p class=\"mb-4\"><strong>Why DevOps Matters?</strong> Manual deployments are error-prone, slow, and don't scale. DevOps automation enables teams to deploy multiple times per day with confidence, roll back instantly if issues arise, and maintain consistent environments from development to production.</p>\n    <p class=\"mb-4\">This tutorial demonstrates how to define serverless infrastructure as code with sam.. You'll learn CI/CD pipeline design, automated testing, deployment strategies (blue/green, canary), rollback procedures, and monitoring best practices.</p>\n    <p>These patterns are used by high-performing engineering teams to achieve deployment frequencies measured in minutes, not weeks, while maintaining 99.99% uptime.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing aws sam template basics, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir sam_template_basics\ncd sam_template_basics\n\n# Initialize SAM project\nsam init --runtime python3.11 --name sam-template-basics\n\n# Project structure:\n# sam-template-basics/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for aws sam template basics. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for AWS SAM Template Basics.\n    \n    This function implements define serverless infrastructure as code with sam..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for aws sam template basics: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: AWS SAM Template Basics - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: sam-template-basics\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Define serverless infrastructure as code with SAM.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /sam-template-basics\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: sam-template-basics\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/sam-template-basics'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: sam-template-basics-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name sam-template-basics-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name sam-template-basics-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name sam-template-basics-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=sam-template-basics-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name sam-template-basics-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for aws sam template basics.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name sam-template-basics-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name sam-template-basics-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['cicd-codepipeline'] = {
  "title": "CI/CD with CodePipeline",
  "category": "DevOps",
  "difficulty": "Intermediate",
  "duration": "15 min",
  "description": "Automate deployments with AWS CodePipeline.",
  "whatYouLearn": [
    "Understand ci/cd with codepipeline architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "CodePipeline",
    "CI/CD",
    "GitHub",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to CI/CD with CodePipeline</h2>\n    <p class=\"mb-4\">Modern DevOps practices emphasize automation, Infrastructure as Code, continuous integration/deployment, and comprehensive monitoring. AWS provides a complete suite of tools for implementing DevOps best practices at scale.</p>\n    <p class=\"mb-4\"><strong>Why DevOps Matters?</strong> Manual deployments are error-prone, slow, and don't scale. DevOps automation enables teams to deploy multiple times per day with confidence, roll back instantly if issues arise, and maintain consistent environments from development to production.</p>\n    <p class=\"mb-4\">This tutorial demonstrates how to automate deployments with aws codepipeline.. You'll learn CI/CD pipeline design, automated testing, deployment strategies (blue/green, canary), rollback procedures, and monitoring best practices.</p>\n    <p>These patterns are used by high-performing engineering teams to achieve deployment frequencies measured in minutes, not weeks, while maintaining 99.99% uptime.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing ci/cd with codepipeline, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir cicd_codepipeline\ncd cicd_codepipeline\n\n# Initialize SAM project\nsam init --runtime python3.11 --name cicd-codepipeline\n\n# Project structure:\n# cicd-codepipeline/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for ci/cd with codepipeline. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for CI/CD with CodePipeline.\n    \n    This function implements automate deployments with aws codepipeline..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for ci/cd with codepipeline: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: CI/CD with CodePipeline - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: cicd-codepipeline\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Automate deployments with AWS CodePipeline.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /cicd-codepipeline\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: cicd-codepipeline\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/cicd-codepipeline'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: cicd-codepipeline-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name cicd-codepipeline-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name cicd-codepipeline-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name cicd-codepipeline-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=cicd-codepipeline-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name cicd-codepipeline-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for ci/cd with codepipeline.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name cicd-codepipeline-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name cicd-codepipeline-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['cloudwatch-monitoring'] = {
  "title": "Monitor Lambda with CloudWatch",
  "category": "DevOps",
  "difficulty": "Beginner",
  "duration": "8 min",
  "description": "Set up logs, metrics, and alarms for Lambda functions.",
  "whatYouLearn": [
    "Understand monitor lambda with cloudwatch architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "CloudWatch",
    "Monitoring",
    "Alarms",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Monitor Lambda with CloudWatch</h2>\n    <p class=\"mb-4\">Modern DevOps practices emphasize automation, Infrastructure as Code, continuous integration/deployment, and comprehensive monitoring. AWS provides a complete suite of tools for implementing DevOps best practices at scale.</p>\n    <p class=\"mb-4\"><strong>Why DevOps Matters?</strong> Manual deployments are error-prone, slow, and don't scale. DevOps automation enables teams to deploy multiple times per day with confidence, roll back instantly if issues arise, and maintain consistent environments from development to production.</p>\n    <p class=\"mb-4\">This tutorial demonstrates how to set up logs, metrics, and alarms for lambda functions.. You'll learn CI/CD pipeline design, automated testing, deployment strategies (blue/green, canary), rollback procedures, and monitoring best practices.</p>\n    <p>These patterns are used by high-performing engineering teams to achieve deployment frequencies measured in minutes, not weeks, while maintaining 99.99% uptime.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing monitor lambda with cloudwatch, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir cloudwatch_monitoring\ncd cloudwatch_monitoring\n\n# Initialize SAM project\nsam init --runtime python3.11 --name cloudwatch-monitoring\n\n# Project structure:\n# cloudwatch-monitoring/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for monitor lambda with cloudwatch. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Monitor Lambda with CloudWatch.\n    \n    This function implements set up logs, metrics, and alarms for lambda functions..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for monitor lambda with cloudwatch: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Monitor Lambda with CloudWatch - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: cloudwatch-monitoring\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Set up logs, metrics, and alarms for Lambda functions.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /cloudwatch-monitoring\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: cloudwatch-monitoring\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/cloudwatch-monitoring'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: cloudwatch-monitoring-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name cloudwatch-monitoring-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name cloudwatch-monitoring-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name cloudwatch-monitoring-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=cloudwatch-monitoring-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name cloudwatch-monitoring-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for monitor lambda with cloudwatch.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name cloudwatch-monitoring-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name cloudwatch-monitoring-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['python-lambda-basics'] = {
  "title": "Python Lambda Function Essentials",
  "category": "Programming",
  "difficulty": "Beginner",
  "duration": "10 min",
  "description": "Master Python Lambda development with best practices.",
  "whatYouLearn": [
    "Understand python lambda function essentials architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Python",
    "Lambda",
    "boto3",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Python Lambda Function Essentials</h2>\n    <p class=\"mb-4\">AWS Lambda supports multiple programming languages including Python, Node.js, Java, Go, .NET, and Ruby. Each language has unique strengths, performance characteristics, and ecosystem libraries that make it suitable for different use cases.</p>\n    <p class=\"mb-4\"><strong>Why This Language?</strong> Choosing the right programming language for Lambda functions impacts cold start times, execution performance, development velocity, and operational costs. Understanding language-specific patterns and best practices is crucial for building efficient serverless applications.</p>\n    <p class=\"mb-4\">In this tutorial, you'll master master python lambda development with best practices.. We'll cover function structure, dependency management, async patterns, error handling, testing strategies, and performance optimization techniques specific to this runtime.</p>\n    <p>The code patterns you learn here apply to building APIs, data processing pipelines, event handlers, and any serverless workload requiring this programming language.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing python lambda function essentials, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir python_lambda_basics\ncd python_lambda_basics\n\n# Initialize SAM project\nsam init --runtime python3.11 --name python-lambda-basics\n\n# Project structure:\n# python-lambda-basics/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for python lambda function essentials. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Python Lambda Function Essentials.\n    \n    This function implements master python lambda development with best practices..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for python lambda function essentials: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Python Lambda Function Essentials - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: python-lambda-basics\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Master Python Lambda development with best practices.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /python-lambda-basics\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: python-lambda-basics\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/python-lambda-basics'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: python-lambda-basics-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name python-lambda-basics-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name python-lambda-basics-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name python-lambda-basics-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=python-lambda-basics-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name python-lambda-basics-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for python lambda function essentials.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name python-lambda-basics-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name python-lambda-basics-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['javascript-lambda-nodejs'] = {
  "title": "Node.js Lambda with AWS SDK",
  "category": "Programming",
  "difficulty": "Beginner",
  "duration": "10 min",
  "description": "Build Lambda functions using JavaScript and AWS SDK v3.",
  "whatYouLearn": [
    "Understand node.js lambda with aws sdk architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "JavaScript",
    "Node.js",
    "Lambda",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Node.js Lambda with AWS SDK</h2>\n    <p class=\"mb-4\">AWS Lambda supports multiple programming languages including Python, Node.js, Java, Go, .NET, and Ruby. Each language has unique strengths, performance characteristics, and ecosystem libraries that make it suitable for different use cases.</p>\n    <p class=\"mb-4\"><strong>Why This Language?</strong> Choosing the right programming language for Lambda functions impacts cold start times, execution performance, development velocity, and operational costs. Understanding language-specific patterns and best practices is crucial for building efficient serverless applications.</p>\n    <p class=\"mb-4\">In this tutorial, you'll master build lambda functions using javascript and aws sdk v3.. We'll cover function structure, dependency management, async patterns, error handling, testing strategies, and performance optimization techniques specific to this runtime.</p>\n    <p>The code patterns you learn here apply to building APIs, data processing pipelines, event handlers, and any serverless workload requiring this programming language.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing node.js lambda with aws sdk, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir javascript_lambda_nodejs\ncd javascript_lambda_nodejs\n\n# Initialize SAM project\nsam init --runtime python3.11 --name javascript-lambda-nodejs\n\n# Project structure:\n# javascript-lambda-nodejs/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for node.js lambda with aws sdk. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Node.js Lambda with AWS SDK.\n    \n    This function implements build lambda functions using javascript and aws sdk v3..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for node.js lambda with aws sdk: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Node.js Lambda with AWS SDK - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: javascript-lambda-nodejs\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Build Lambda functions using JavaScript and AWS SDK v3.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /javascript-lambda-nodejs\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: javascript-lambda-nodejs\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/javascript-lambda-nodejs'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: javascript-lambda-nodejs-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name javascript-lambda-nodejs-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name javascript-lambda-nodejs-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name javascript-lambda-nodejs-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=javascript-lambda-nodejs-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name javascript-lambda-nodejs-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for node.js lambda with aws sdk.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name javascript-lambda-nodejs-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name javascript-lambda-nodejs-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['java-lambda-spring'] = {
  "title": "Java Lambda with Spring Boot",
  "category": "Programming",
  "difficulty": "Intermediate",
  "duration": "18 min",
  "description": "Build enterprise Lambda functions using Java and Spring.",
  "whatYouLearn": [
    "Understand java lambda with spring boot architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Java",
    "Spring Boot",
    "Lambda",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Java Lambda with Spring Boot</h2>\n    <p class=\"mb-4\">AWS Lambda supports multiple programming languages including Python, Node.js, Java, Go, .NET, and Ruby. Each language has unique strengths, performance characteristics, and ecosystem libraries that make it suitable for different use cases.</p>\n    <p class=\"mb-4\"><strong>Why This Language?</strong> Choosing the right programming language for Lambda functions impacts cold start times, execution performance, development velocity, and operational costs. Understanding language-specific patterns and best practices is crucial for building efficient serverless applications.</p>\n    <p class=\"mb-4\">In this tutorial, you'll master build enterprise lambda functions using java and spring.. We'll cover function structure, dependency management, async patterns, error handling, testing strategies, and performance optimization techniques specific to this runtime.</p>\n    <p>The code patterns you learn here apply to building APIs, data processing pipelines, event handlers, and any serverless workload requiring this programming language.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing java lambda with spring boot, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir java_lambda_spring\ncd java_lambda_spring\n\n# Initialize SAM project\nsam init --runtime python3.11 --name java-lambda-spring\n\n# Project structure:\n# java-lambda-spring/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for java lambda with spring boot. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Java Lambda with Spring Boot.\n    \n    This function implements build enterprise lambda functions using java and spring..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for java lambda with spring boot: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Java Lambda with Spring Boot - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: java-lambda-spring\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Build enterprise Lambda functions using Java and Spring.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /java-lambda-spring\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: java-lambda-spring\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/java-lambda-spring'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: java-lambda-spring-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name java-lambda-spring-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name java-lambda-spring-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name java-lambda-spring-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=java-lambda-spring-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name java-lambda-spring-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for java lambda with spring boot.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name java-lambda-spring-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name java-lambda-spring-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['python-async-lambda'] = {
  "title": "Async Python in Lambda",
  "category": "Programming",
  "difficulty": "Intermediate",
  "duration": "12 min",
  "description": "Use asyncio for concurrent operations in Lambda.",
  "whatYouLearn": [
    "Understand async python in lambda architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "Python",
    "Async",
    "Performance",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to Async Python in Lambda</h2>\n    <p class=\"mb-4\">AWS Lambda supports multiple programming languages including Python, Node.js, Java, Go, .NET, and Ruby. Each language has unique strengths, performance characteristics, and ecosystem libraries that make it suitable for different use cases.</p>\n    <p class=\"mb-4\"><strong>Why This Language?</strong> Choosing the right programming language for Lambda functions impacts cold start times, execution performance, development velocity, and operational costs. Understanding language-specific patterns and best practices is crucial for building efficient serverless applications.</p>\n    <p class=\"mb-4\">In this tutorial, you'll master use asyncio for concurrent operations in lambda.. We'll cover function structure, dependency management, async patterns, error handling, testing strategies, and performance optimization techniques specific to this runtime.</p>\n    <p>The code patterns you learn here apply to building APIs, data processing pipelines, event handlers, and any serverless workload requiring this programming language.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing async python in lambda, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir python_async_lambda\ncd python_async_lambda\n\n# Initialize SAM project\nsam init --runtime python3.11 --name python-async-lambda\n\n# Project structure:\n# python-async-lambda/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for async python in lambda. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for Async Python in Lambda.\n    \n    This function implements use asyncio for concurrent operations in lambda..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for async python in lambda: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Async Python in Lambda - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: python-async-lambda\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Use asyncio for concurrent operations in Lambda.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /python-async-lambda\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: python-async-lambda\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/python-async-lambda'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: python-async-lambda-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name python-async-lambda-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name python-async-lambda-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name python-async-lambda-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=python-async-lambda-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name python-async-lambda-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for async python in lambda.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name python-async-lambda-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name python-async-lambda-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

window.TUTORIALS['typescript-lambda-cdk'] = {
  "title": "TypeScript Lambda with AWS CDK",
  "category": "Programming",
  "difficulty": "Intermediate",
  "duration": "15 min",
  "description": "Build type-safe Lambda functions using TypeScript and CDK.",
  "whatYouLearn": [
    "Understand typescript lambda with aws cdk architecture and design patterns",
    "Implement production-ready code with error handling and logging",
    "Deploy using Infrastructure as Code (SAM/CloudFormation)",
    "Configure monitoring, alarms, and observability",
    "Optimize for performance, cost, and security",
    "Test and validate your implementation"
  ],
  "skillsImproved": [
    "TypeScript",
    "CDK",
    "Lambda",
    "AWS",
    "Cloud Architecture",
    "Best Practices"
  ],
  "architecture": "graph LR\n    A[User/Client] -->|Request| B[AWS Service]\n    B -->|Process| C[Lambda/Compute]\n    C -->|Store/Retrieve| D[Data Layer]\n    D -->|Response| C\n    C -->|Result| B\n    B -->|Response| A",
  "intro": "<h2 class=\"text-xl font-bold mb-3\">Introduction to TypeScript Lambda with AWS CDK</h2>\n    <p class=\"mb-4\">AWS Lambda supports multiple programming languages including Python, Node.js, Java, Go, .NET, and Ruby. Each language has unique strengths, performance characteristics, and ecosystem libraries that make it suitable for different use cases.</p>\n    <p class=\"mb-4\"><strong>Why This Language?</strong> Choosing the right programming language for Lambda functions impacts cold start times, execution performance, development velocity, and operational costs. Understanding language-specific patterns and best practices is crucial for building efficient serverless applications.</p>\n    <p class=\"mb-4\">In this tutorial, you'll master build type-safe lambda functions using typescript and cdk.. We'll cover function structure, dependency management, async patterns, error handling, testing strategies, and performance optimization techniques specific to this runtime.</p>\n    <p>The code patterns you learn here apply to building APIs, data processing pipelines, event handlers, and any serverless workload requiring this programming language.</p>",
  "steps": [
    {
      "title": "Step 1: Architecture Overview and Setup",
      "content": "Before implementing typescript lambda with aws cdk, it's crucial to understand the architecture and set up your development environment. This step covers the AWS services involved, their interactions, data flow, security considerations, and local development setup. We'll install necessary tools (AWS CLI, SAM CLI, SDKs) and configure credentials for deployment.",
      "language": "bash",
      "code": "# Install AWS CLI\n# macOS: brew install awscli\n# Windows: Download from aws.amazon.com/cli\n# Linux: pip install awscli\n\n# Configure AWS credentials\naws configure\n# Enter: Access Key ID, Secret Access Key, Region (us-east-1), Output format (json)\n\n# Install AWS SAM CLI for serverless deployments\n# macOS: brew install aws-sam-cli\n# Windows: choco install aws-sam-cli\n# Linux: pip install aws-sam-cli\n\n# Verify installations\naws --version\nsam --version\n\n# Create project directory\nmkdir typescript_lambda_cdk\ncd typescript_lambda_cdk\n\n# Initialize SAM project\nsam init --runtime python3.11 --name typescript-lambda-cdk\n\n# Project structure:\n# typescript-lambda-cdk/\n# \u251c\u2500\u2500 template.yaml          # SAM template (Infrastructure as Code)\n# \u251c\u2500\u2500 functions/             # Lambda function code\n# \u2502   \u2514\u2500\u2500 app.py\n# \u251c\u2500\u2500 tests/                 # Unit and integration tests\n# \u251c\u2500\u2500 requirements.txt       # Python dependencies\n# \u2514\u2500\u2500 README.md"
    },
    {
      "title": "Step 2: Implement Core Functionality",
      "content": "Now we'll implement the core business logic for typescript lambda with aws cdk. This includes writing the main function code, handling inputs/outputs, implementing error handling, adding logging for debugging, and following AWS best practices for security and performance. The code is production-ready with proper exception handling and structured logging.",
      "language": "python",
      "code": "import json\nimport boto3\nimport os\nimport logging\nfrom datetime import datetime\n\n# Configure structured logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize AWS clients (reused across invocations)\n# Client initialization outside handler for connection reuse\nclient = boto3.client('service-name')  # Replace with actual service\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Main Lambda handler for TypeScript Lambda with AWS CDK.\n    \n    This function implements build type-safe lambda functions using typescript and cdk..\n    It follows AWS best practices for error handling, logging,\n    and performance optimization.\n    \n    Args:\n        event: Event data from trigger (API Gateway, S3, etc.)\n        context: Lambda context with runtime information\n    \n    Returns:\n        dict: Response with statusCode, headers, and body\n    \"\"\"\n    # Log incoming event for debugging\n    logger.info(f\"Received event: {json.dumps(event)}\")\n    \n    try:\n        # Extract and validate input\n        input_data = extract_input(event)\n        validate_input(input_data)\n        \n        # Process business logic\n        result = process_request(input_data)\n        \n        # Log success\n        logger.info(f\"Successfully processed request: {result}\")\n        \n        # Return success response\n        return create_response(200, {\n            'success': True,\n            'data': result,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n        \n    except ValueError as e:\n        # Handle validation errors\n        logger.error(f\"Validation error: {str(e)}\")\n        return create_response(400, {\n            'success': False,\n            'error': 'Invalid input',\n            'message': str(e)\n        })\n        \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n        return create_response(500, {\n            'success': False,\n            'error': 'Internal server error',\n            'message': 'An unexpected error occurred'\n        })\n\ndef extract_input(event):\n    \"\"\"Extract input from event based on trigger type\"\"\"\n    # Handle API Gateway events\n    if 'body' in event:\n        return json.loads(event['body'])\n    # Handle direct invocations\n    return event\n\ndef validate_input(data):\n    \"\"\"Validate input data\"\"\"\n    required_fields = ['field1', 'field2']  # Customize\n    for field in required_fields:\n        if field not in data:\n            raise ValueError(f\"Missing required field: {field}\")\n\ndef process_request(data):\n    \"\"\"Core business logic implementation\"\"\"\n    # Implement your business logic here\n    # This is where the main work happens\n    result = {\n        'processed': True,\n        'input': data,\n        'output': 'Processed successfully'\n    }\n    return result\n\ndef create_response(status_code, body):\n    \"\"\"Create standardized API response\"\"\"\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n            'Access-Control-Allow-Origin': '*'\n        },\n        'body': json.dumps(body)\n    }"
    },
    {
      "title": "Step 3: Define Infrastructure with SAM",
      "content": "Infrastructure as Code (IaC) is essential for reproducible deployments. This SAM template defines all AWS resources needed for typescript lambda with aws cdk: Lambda functions, IAM roles, event triggers, and monitoring. SAM simplifies CloudFormation syntax and automatically handles common serverless patterns like API Gateway integration and Lambda permissions.",
      "language": "yaml",
      "code": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: TypeScript Lambda with AWS CDK - Production-ready serverless application\n\n# Global configuration for all functions\nGlobals:\n  Function:\n    Runtime: python3.11\n    Timeout: 30\n    MemorySize: 512\n    Environment:\n      Variables:\n        LOG_LEVEL: INFO\n        POWERTOOLS_SERVICE_NAME: typescript-lambda-cdk\n    Tracing: Active  # Enable X-Ray tracing\n\nParameters:\n  Environment:\n    Type: String\n    Default: prod\n    AllowedValues: [dev, staging, prod]\n    Description: Deployment environment\n\nResources:\n  # Main Lambda Function\n  MainFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: app.lambda_handler\n      Description: Build type-safe Lambda functions using TypeScript and CDK.\n      # IAM permissions\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - logs:CreateLogGroup\n                - logs:CreateLogStream\n                - logs:PutLogEvents\n              Resource: '*'\n      # Event triggers (customize based on use case)\n      Events:\n        ApiEvent:\n          Type: Api\n          Properties:\n            Path: /typescript-lambda-cdk\n            Method: POST\n      # Tags for cost tracking\n      Tags:\n        Environment: !Ref Environment\n        Project: typescript-lambda-cdk\n\n  # CloudWatch Log Group with retention\n  FunctionLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub '/aws/lambda/${MainFunction}'\n      RetentionInDays: 7\n\n  # CloudWatch Alarm for errors\n  ErrorAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-errors'\n      AlarmDescription: Alert on Lambda errors\n      MetricName: Errors\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 5\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\n  # CloudWatch Alarm for throttles\n  ThrottleAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Sub '${AWS::StackName}-throttles'\n      AlarmDescription: Alert on Lambda throttles\n      MetricName: Throttles\n      Namespace: AWS/Lambda\n      Statistic: Sum\n      Period: 300\n      EvaluationPeriods: 1\n      Threshold: 10\n      ComparisonOperator: GreaterThanThreshold\n      Dimensions:\n        - Name: FunctionName\n          Value: !Ref MainFunction\n\nOutputs:\n  FunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MainFunction.Arn\n    Export:\n      Name: !Sub '${AWS::StackName}-FunctionArn'\n  \n  ApiUrl:\n    Description: API Gateway endpoint URL\n    Value: !Sub 'https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/typescript-lambda-cdk'\n    Export:\n      Name: !Sub '${AWS::StackName}-ApiUrl'"
    },
    {
      "title": "Step 4: Deploy and Test",
      "content": "Deployment automation ensures consistent, repeatable releases. We'll use SAM CLI to build, package, and deploy the application. Testing includes unit tests, integration tests, and end-to-end validation. We'll also verify monitoring dashboards and alarms are working correctly.",
      "language": "bash",
      "code": "# Build the application\n# This packages code and resolves dependencies\nsam build\n\n# Run local tests before deployment\npython -m pytest tests/ -v\n\n# Deploy to AWS (first time with --guided)\nsam deploy --guided\n# Prompts:\n# - Stack Name: typescript-lambda-cdk-stack\n# - AWS Region: us-east-1\n# - Confirm changes: Y\n# - Allow SAM CLI IAM role creation: Y\n# - Save arguments to config: Y\n\n# Subsequent deployments (uses saved config)\nsam deploy\n\n# Get outputs (API URL, Function ARN, etc.)\naws cloudformation describe-stacks \\\n  --stack-name typescript-lambda-cdk-stack \\\n  --query 'Stacks[0].Outputs' \\\n  --output table\n\n# Test the deployed function\nAPI_URL=$(aws cloudformation describe-stacks \\\n  --stack-name typescript-lambda-cdk-stack \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \\\n  --output text)\n\n# Send test request\ncurl -X POST $API_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"field1\": \"value1\", \"field2\": \"value2\"}'\n\n# View real-time logs\nsam logs --stack-name typescript-lambda-cdk-stack --tail\n\n# View CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=typescript-lambda-cdk-stack-MainFunction-XXXXX \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\\n  --period 300 \\\n  --statistics Sum\n\n# Load test (optional)\nfor i in {1..100}; do\n  curl -X POST $API_URL \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"field1\": \"test$i\", \"field2\": \"value$i\"}' &\ndone\nwait\n\n# Clean up resources when done\nsam delete --stack-name typescript-lambda-cdk-stack"
    },
    {
      "title": "Step 5: Monitor, Optimize, and Scale",
      "content": "Production applications require comprehensive monitoring, performance optimization, and cost management. This step covers CloudWatch dashboards, X-Ray tracing, performance tuning, cost optimization strategies, and scaling considerations for typescript lambda with aws cdk.",
      "language": "bash",
      "code": "# Create CloudWatch Dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name typescript-lambda-cdk-dashboard \\\n  --dashboard-body file://dashboard.json\n\n# Enable detailed monitoring\naws lambda update-function-configuration \\\n  --function-name typescript-lambda-cdk-stack-MainFunction-XXXXX \\\n  --tracing-config Mode=Active\n\n# View X-Ray traces for performance analysis\naws xray get-trace-summaries \\\n  --start-time $(date -u -d '1 hour ago' +%s) \\\n  --end-time $(date -u +%s)\n\n# Performance Optimization Tips:\n# 1. Increase memory (more memory = more CPU)\n#    - Test with 512MB, 1024MB, 1536MB\n#    - Find sweet spot for cost vs performance\n# 2. Use Lambda Layers for dependencies\n#    - Reduces deployment package size\n#    - Faster cold starts\n# 3. Enable Provisioned Concurrency for consistent latency\n#    - Eliminates cold starts\n#    - Higher cost but predictable performance\n# 4. Optimize code\n#    - Minimize imports\n#    - Reuse connections\n#    - Cache data when possible\n\n# Cost Optimization:\n# 1. Right-size memory allocation\n# 2. Set appropriate timeout (don't use default 3s if you need 10s)\n# 3. Use reserved concurrency to prevent runaway costs\n# 4. Enable CloudWatch Logs Insights for debugging instead of verbose logging\n\n# View cost breakdown\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics BlendedCost \\\n  --filter file://cost-filter.json\n\n# Scaling Considerations:\n# - Lambda auto-scales to 1000 concurrent executions (default)\n# - Request service limit increase if needed\n# - Use SQS for buffering during traffic spikes\n# - Implement exponential backoff for retries\n# - Monitor throttling metrics\n\n# Best Practices Checklist:\n# \u2705 Error handling and retries implemented\n# \u2705 Structured logging with correlation IDs\n# \u2705 CloudWatch alarms configured\n# \u2705 X-Ray tracing enabled\n# \u2705 IAM roles follow least privilege\n# \u2705 Secrets stored in Secrets Manager/Parameter Store\n# \u2705 Unit and integration tests passing\n# \u2705 CI/CD pipeline configured\n# \u2705 Documentation updated\n# \u2705 Cost alerts configured"
    }
  ]
};

